{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed61d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy loaded from /home/sachin/IsaacLab/logs/rsl_rl/rexai_direct/2025-10-04_20-00-30/exported/policy.onnx\n",
      "Input: obs, Output: actions\n",
      "Observation shape: (42,)\n",
      "Action shape: (10,)\n",
      "Action values: [-1.4317504   0.64814943 -0.4952938   1.8832533   1.7667413  -0.8991956\n",
      " -2.835862    3.8421986  -3.004331   -6.2876744 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "class ONNXPolicy:\n",
    "    def __init__(self, onnx_path: str, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize ONNX policy\n",
    "        \n",
    "        Args:\n",
    "            onnx_path: Path to the .onnx policy file\n",
    "            device: Device to run inference on (\"cpu\" or \"cuda\")\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "\n",
    "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == \"cuda\" else ['CPUExecutionProvider']\n",
    "        self.session = ort.InferenceSession(onnx_path, providers=providers)\n",
    "\n",
    "        self.input_name = self.session.get_inputs()[0].name\n",
    "        self.output_name = self.session.get_outputs()[0].name\n",
    "        \n",
    "        print(f\"Policy loaded from {onnx_path}\")\n",
    "        print(f\"Input: {self.input_name}, Output: {self.output_name}\")\n",
    "    \n",
    "    def __call__(self, observation: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Run policy inference\n",
    "        \n",
    "        Args:\n",
    "            observation: Observation array of shape (42,) or (batch, 42)\n",
    "        \n",
    "        Returns:\n",
    "            Action array of shape (10,) or (batch, 10)\n",
    "        \"\"\"\n",
    "        # observation is 2D (batch, features)\n",
    "        if observation.ndim == 1:\n",
    "            observation = observation.reshape(1, -1)\n",
    "\n",
    "        observation = observation.astype(np.float32)\n",
    "        \n",
    "        # Run inference\n",
    "        action = self.session.run([self.output_name], {self.input_name: observation})[0]\n",
    "        \n",
    "        if action.shape[0] == 1:\n",
    "            return action[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    policy = ONNXPolicy(\"/path/to/policy.onnx\", device=\"cuda\")\n",
    "    obs = np.random.randn(42).astype(np.float32)\n",
    "\n",
    "    action = policy(obs)\n",
    "    print(f\"Observation shape: {obs.shape}\")\n",
    "    print(f\"Action shape: {action.shape}\")\n",
    "    print(f\"Action values: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a3c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as TorchScript model\n",
      "Policy loaded from /home/sachin/IsaacLab/logs/rsl_rl/rexai_direct/2025-10-04_20-00-30/exported/policy.pt\n",
      "Model on device: cuda\n",
      "Num observations: 42, Num actions: 10\n",
      "TorchScript model: True\n",
      "\n",
      "Single Observation Test:\n",
      "Observation shape: (42,)\n",
      "Action shape: (10,)\n",
      "Action values: [ -2.6916864    1.2083182   -0.17800936   2.9344132    3.8472185\n",
      "  -1.2177778   -1.5107269    1.573474    -7.824546   -12.670531  ]\n",
      "Action range: [-12.671, 3.847]\n",
      "\n",
      "Batch Observation Test:\n",
      "Batch observation shape: (4, 42)\n",
      "Batch action shape: (4, 10)\n",
      "\n",
      "Model type: TorchScript\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Union, Tuple\n",
    "\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "    \"\"\"Actor-Critic policy for Torch\"\"\"\n",
    "    \n",
    "    def __init__(self, num_obs: int = 42, num_actions: int = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_obs, 400),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Linear(100, num_actions)\n",
    "        )\n",
    "        \n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_obs, 400),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ELU(alpha=1.0),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass - returns actions\"\"\"\n",
    "        return self.actor(obs)\n",
    "    \n",
    "    def act(self, obs: torch.Tensor, deterministic: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Get action from observation\"\"\"\n",
    "        action = self.actor(obs)\n",
    "        return torch.tanh(action)  # Apply tanh for bounded actions\n",
    "    \n",
    "    def evaluate(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get both action and value estimate\"\"\"\n",
    "        action = self.actor(obs)\n",
    "        value = self.critic(obs)\n",
    "        return action, value\n",
    "\n",
    "\n",
    "class PolicyWrapper:    \n",
    "    def __init__(self, checkpoint_path: str, num_obs: int = 42, num_actions: int = 10, \n",
    "                 device: str = \"cpu\", action_scale: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize policy from checkpoint\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to .pt checkpoint file\n",
    "            num_obs: Number of observation features\n",
    "            num_actions: Number of actions\n",
    "            device: Device to run on (\"cpu\" or \"cuda\")\n",
    "            action_scale: Scaling factor for actions\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.action_scale = action_scale\n",
    "        self.num_obs = num_obs\n",
    "        self.num_actions = num_actions\n",
    "        self.is_torchscript = False\n",
    "        \n",
    "        self.model = self.load_checkpoint(checkpoint_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Policy loaded from {checkpoint_path}\")\n",
    "        print(f\"Model on device: {self.device}\")\n",
    "        print(f\"Num observations: {num_obs}, Num actions: {num_actions}\")\n",
    "        print(f\"TorchScript model: {self.is_torchscript}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str):\n",
    "        \"\"\"Load model weights from checkpoint - handles both TorchScript and state_dict\"\"\"\n",
    "        try:\n",
    "            model = torch.jit.load(checkpoint_path, map_location=self.device)\n",
    "            self.is_torchscript = True\n",
    "            print(\"Loaded as TorchScript model\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Not a TorchScript model, trying as state_dict: {e}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            model = ActorCriticPolicy(self.num_obs, self.num_actions).to(self.device)\n",
    "\n",
    "            if isinstance(checkpoint, dict):\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                elif 'actor_critic' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['actor_critic'])\n",
    "                elif 'state_dict' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['state_dict'])\n",
    "                else:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported checkpoint format\")\n",
    "            \n",
    "            print(\"Loaded as state_dict\")\n",
    "            return model\n",
    "    \n",
    "    def __call__(self, observation: Union[np.ndarray, torch.Tensor]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get action from observation\n",
    "        Args:\n",
    "            observation: Observation of shape (num_obs,) or (batch, num_obs)\n",
    "        Returns:\n",
    "            Action array of shape (10,) or (batch, 10)\n",
    "        \"\"\"\n",
    "        # Convert to tensor if needed\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            obs_tensor = torch.from_numpy(observation).float()\n",
    "        else:\n",
    "            obs_tensor = observation.float()\n",
    "        \n",
    "        single_obs = obs_tensor.ndim == 1\n",
    "        if single_obs:\n",
    "            obs_tensor = obs_tensor.unsqueeze(0)\n",
    "        \n",
    "        if obs_tensor.shape[1] != self.num_obs:\n",
    "            obs_tensor = obs_tensor[:, :self.num_obs]\n",
    "        \n",
    "        # Move to device\n",
    "        obs_tensor = obs_tensor.to(self.device)\n",
    "        \n",
    "        # Get action\n",
    "        with torch.no_grad():\n",
    "            if self.is_torchscript:\n",
    "                action = self.model(obs_tensor)\n",
    "            else:\n",
    "                action = self.model.act(obs_tensor, deterministic=True)\n",
    "            \n",
    "            action = action * self.action_scale\n",
    "        \n",
    "        action = action.cpu().numpy()\n",
    "\n",
    "        if single_obs:\n",
    "            return action[0]\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_value(self, observation: Union[np.ndarray, torch.Tensor]) -> np.ndarray:\n",
    "        \"\"\"Get value estimate for observation (only for non-TorchScript models)\"\"\"\n",
    "        if self.is_torchscript:\n",
    "            raise NotImplementedError(\"Value estimation not available for TorchScript models. \"\n",
    "                                     \"TorchScript model only exports the actor network.\")\n",
    "        \n",
    "        if isinstance(observation, np.ndarray):\n",
    "            obs_tensor = torch.from_numpy(observation).float()\n",
    "        else:\n",
    "            obs_tensor = observation.float()\n",
    "        \n",
    "        single_obs = obs_tensor.ndim == 1\n",
    "        if single_obs:\n",
    "            obs_tensor = obs_tensor.unsqueeze(0)\n",
    "        \n",
    "        if obs_tensor.shape[1] != self.num_obs:\n",
    "            obs_tensor = obs_tensor[:, :self.num_obs]\n",
    "        \n",
    "        obs_tensor = obs_tensor.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            value = self.model.critic(obs_tensor)\n",
    "        \n",
    "        value = value.cpu().numpy()\n",
    "        \n",
    "        if single_obs:\n",
    "            return value[0, 0]\n",
    "        \n",
    "        return value.squeeze(-1)\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    policy = PolicyWrapper(\n",
    "        checkpoint_path=\"/pat/to/policy.pt\",\n",
    "        num_obs=42,\n",
    "        num_actions=10,\n",
    "        device=\"cuda\",\n",
    "        action_scale=1.0\n",
    "    )\n",
    "\n",
    "    obs = np.random.randn(42).astype(np.float32)\n",
    "    action = policy(obs)\n",
    "    \n",
    "    print(f\"\\nSingle Observation Test:\")\n",
    "    print(f\"Observation shape: {obs.shape}\")\n",
    "    print(f\"Action shape: {action.shape}\")\n",
    "    print(f\"Action values: {action}\")\n",
    "    print(f\"Action range: [{action.min():.3f}, {action.max():.3f}]\")\n",
    "    \n",
    "    batch_obs = np.random.randn(4, 42).astype(np.float32)\n",
    "    batch_actions = policy(batch_obs)\n",
    "    \n",
    "    print(f\"\\nBatch Observation Test:\")\n",
    "    print(f\"Batch observation shape: {batch_obs.shape}\")\n",
    "    print(f\"Batch action shape: {batch_actions.shape}\")\n",
    "\n",
    "    # Print model info\n",
    "    print(f\"\\nModel type: {'TorchScript' if policy.is_torchscript else 'PyTorch'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
